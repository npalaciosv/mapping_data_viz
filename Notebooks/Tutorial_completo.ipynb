{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Mapeando datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚û°Ô∏è Apuntar\n",
    "\n",
    "üóíÔ∏è **Observaciones:**\n",
    "\n",
    "‚ö†Ô∏è **Alertas:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Importamos las respectivas librer√≠as**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import folium\n",
    "import sqlite3\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Descomprimimos los archivos correspondientes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚û°Ô∏è Creamos una variable path que nos ayude a indentificar donde estan todos los archivos zip.\n",
    "\n",
    "üóíÔ∏è **Observaciones:** Vamos a utilizar la metodologia del staging, que para este caso ser√° mantener los archivos crudos en la carpeta raw y, cuando ya est√©n transformados, los vamos a localizar en la carpeta silver. Esta informacion proviene deL DANE üîó https://microdatos.dane.gov.co/index.php/catalog/697/get-microdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = '/home/npalaciosv/Catedra/Geoanalitycs/src/Data/Raw/'\n",
    "silver_path = '/home/npalaciosv/Catedra/Geoanalitycs/src/Data/Silver/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚û°Ô∏è Para poder recorrer los archivos vamos a utilizar la libreria *\"Path\"*, la cual es m√°s sensilla de usar que la libreria *\"Os\"*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un objetio Path con la direccion de los archivos crudos\n",
    "data_files = Path(raw_path)\n",
    "\n",
    "# Recorremos cada uno de los archivos usando la funcion rglob('*')\n",
    "for file in data_files.rglob('*'):\n",
    "\n",
    "    # Para evitar que los espacios y el guion bajo afecten algun comando, los eliminamos del nombre del archivo\n",
    "    new_name = file.name.replace(\" \",\"\").replace(\"_\",\"\")\n",
    "\n",
    "    # Ajustamos el path completo ahora con el nuevo nombre\n",
    "    new_path = file.with_name(new_name)\n",
    "\n",
    "    # Con esta funcion renombramos el archivo.\n",
    "    file.rename(new_path)\n",
    "\n",
    "    # Iniciamos el proceso de extraccion de cada archivo\n",
    "    with zipfile.ZipFile(f'{raw_path+file.name}','r') as extraction:\n",
    "\n",
    "        # La informacion sera extraida la carpeta Silver\n",
    "        extraction.extractall(silver_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚û°Ô∏è Como podemos ver, los archivos vienen en varios formatos, sin embargo, a nosotros solo nos interesa aquellos con formato .csv, as√≠ que procedemos a eliminar el resto de archivos.\n",
    "\n",
    "üóíÔ∏è **Observaciones:** Al parecer, algunos archivos se descomprimen directamente y otros quedan venian dentro de una carpeta, as√≠ que nos toca primera trabajar con los archivos que se descomprimieron directamente y luego con los que quedaron dentro de una carpeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminado: /home/npalaciosv/Catedra/Geoanalitycs/src/Data/Silver/2020_SemII.dta\n",
      "Eliminado: /home/npalaciosv/Catedra/Geoanalitycs/src/Data/Silver/2019_SemII.dta\n",
      "Eliminado: /home/npalaciosv/Catedra/Geoanalitycs/src/Data/Silver/2021 ( I Semestre).dta\n",
      "Eliminado: /home/npalaciosv/Catedra/Geoanalitycs/src/Data/Silver/2019_SemII.sav\n",
      "Eliminado: /home/npalaciosv/Catedra/Geoanalitycs/src/Data/Silver/2019_SemI.dta\n",
      "Eliminado: /home/npalaciosv/Catedra/Geoanalitycs/src/Data/Silver/2018_SemI.sav\n",
      "Eliminado: /home/npalaciosv/Catedra/Geoanalitycs/src/Data/Silver/2018_SemII.dta\n",
      "Eliminado: /home/npalaciosv/Catedra/Geoanalitycs/src/Data/Silver/2020_SemI.dta\n",
      "Eliminado: /home/npalaciosv/Catedra/Geoanalitycs/src/Data/Silver/2020_SemII.sav\n",
      "Eliminado: /home/npalaciosv/Catedra/Geoanalitycs/src/Data/Silver/2019_SemI.sav\n",
      "Eliminado: /home/npalaciosv/Catedra/Geoanalitycs/src/Data/Silver/2018_SemII.sav\n",
      "Eliminado: /home/npalaciosv/Catedra/Geoanalitycs/src/Data/Silver/2021 ( I Semestre).sav\n",
      "Eliminado: /home/npalaciosv/Catedra/Geoanalitycs/src/Data/Silver/2022(II Semestre).dta\n",
      "Eliminado: /home/npalaciosv/Catedra/Geoanalitycs/src/Data/Silver/2022(II Semestre).sav\n",
      "Eliminado: /home/npalaciosv/Catedra/Geoanalitycs/src/Data/Silver/2018_SemI.dta\n",
      "Eliminado: /home/npalaciosv/Catedra/Geoanalitycs/src/Data/Silver/2020_SemI.sav\n"
     ]
    }
   ],
   "source": [
    "# Creamos un objetio Path con la direccion de los archivos descomprimidos\n",
    "silver_files = Path(silver_path)\n",
    "\n",
    "# Recorremos cada uno de los archivos para verificar su extension. Usamos la funcion iterdir()\n",
    "for file in silver_files.iterdir():\n",
    "\n",
    "    # Verificamos si es un archivo (ya que puede ser una carpeta) y no termina en '.csv'\n",
    "    if file.is_file() and file.suffix != '.csv':\n",
    "\n",
    "        # Eliminamos el archivo y lo notificamos\n",
    "        file.unlink()\n",
    "        print(f'Eliminado: {file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo: 2021(II Semestre).csv movido exitosamente\n",
      "Carpeta: 2021 (II.semestre) elminada\n",
      "Archivo: SIPSA_A_Isem2022.csv movido exitosamente\n",
      "Carpeta: 2022 ( I Semestre ) elminada\n",
      "Archivo: SIPSA_A Isem2024.csv movido exitosamente\n",
      "Carpeta: 2024 ( I Semestre ) elminada\n",
      "Archivo: SIPSA_A_IIsem2023.csv movido exitosamente\n",
      "Carpeta: 2023 ( II Semestre ) elminada\n",
      "Archivo: SIPSA_A_Isem2023.csv movido exitosamente\n",
      "Carpeta: SIPSA_A_Isem2023 elminada\n"
     ]
    }
   ],
   "source": [
    "# Recorremos cada uno de los archivos para verificar que sea una carpeta\n",
    "for file in silver_files.iterdir():\n",
    "\n",
    "    # Si archivo es realmente un archivo, sin importar el formato, no pasar√° nada, pero si es una carpeta movera el archivo fuera de esta\n",
    "    if file.is_file():\n",
    "        pass\n",
    "    else:\n",
    "        # Nos movemos un nivel mas en el path, es decir, entramos en la carpeta\n",
    "        subfolder = silver_files.joinpath(file.name)\n",
    "\n",
    "        #Recorremos la carpeta\n",
    "        for subfile in subfolder.glob('*'):\n",
    "\n",
    "            # Buscamos los archivos .csv y los movemos a un nivel anterior en el path, es decir, los ponemos al nivel de los anteriores\n",
    "            if subfile.suffix == '.csv':\n",
    "                subfile.replace(silver_path+\"/\"+subfile.name)\n",
    "                print(f'Archivo: {subfile.name} movido exitosamente')\n",
    "\n",
    "        # Eliminamos la carpeta una vez el archivo se ha movido al nivel anterior\n",
    "        shutil.rmtree(subfolder)\n",
    "        print(f'Carpeta: {subfolder.name} elminada')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Unificamos la informacion que necesitamos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Alertas:** Revisando la informacion previamente, nos dimos cuenta que no todos los archivos traen las columnas con el mismo nombre, por lo tanto nos toca hacer una homologacion de nombre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un diccionario que servira para homologar los nombres correspondientes\n",
    "\n",
    "columnas_ajustadas = {\n",
    "    'Fuente':'Mayorista',\n",
    "    'FechaEncuesta':'Fecha',\n",
    "    'Fecha':'Fecha',\n",
    "    'Cod. Depto Proc.':'Codigo_departamento',\n",
    "    'Cod. Municipio Proc.':'Codigo_municipio',\n",
    "    'Departamento Proc.':'Departamento',\n",
    "    'Municipio Proc.':'Municipio',\n",
    "    'Grupo':'Grupo',\n",
    "    'Ali':'Alimento',\n",
    "    'Cant Kg':'Cant_Kg',\n",
    "    'C√≥digo Departamento':'Codigo_departamento',\n",
    "    ' C√≥digo Municipio ':'Codigo_municipio',\n",
    "    'Alimento':'Alimento',\n",
    "    'Cuidad, Mercado Mayorista':'Mayorista',\n",
    "    'Origen':'Origen',\n",
    "    'Unnamed: 9':'Unnamed: 9',\n",
    "    'Unnamed: 10':'Unnamed: 10',\n",
    "    'Codigo CPC':'Codigo CPC'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚û°Ô∏è Unimos todos los archivos en un dataframe unico que nos permitir√° consolidar la informacion...üò® Anticipo que va a ser bien pesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_141162/2693946809.py:8: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_temp = pd.read_csv(file, encoding='unicode_escape', sep=\";\")\n"
     ]
    }
   ],
   "source": [
    "# Creamos un dataframe vacio que nos permitira consolidar todos los archivos\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Recorremos archivo por archivo para irlos consolidando\n",
    "for file in silver_files.iterdir():\n",
    "\n",
    "    # Leemos cada archivo. Usamos 'unicode_escape' ya que es el formato que nos permite leer los archivos sin ningun error\n",
    "    df_temp = pd.read_csv(file, encoding='unicode_escape', sep=\";\")\n",
    "\n",
    "    # Creamos una columna que nos permitira rastear de que archivo esta saliendo la informacion\n",
    "    df_temp['Origen'] = file.name\n",
    "\n",
    "    # Cambiamos el nombre de las columnas de acuerdo con el diccionario que definimos previamente\n",
    "    for column in df_temp.columns:\n",
    "        df_temp.rename(columns={column: columnas_ajustadas[column]}, inplace=True)\n",
    "\n",
    "    # Concatenamos los dataframes\n",
    "    df =pd.concat([df,df_temp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚û°Ô∏è Algunas columnas no traen datos, por tal motivo las vamos a eliminar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 9','Unnamed: 10','Codigo CPC'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Alertas:** Al ser informacion de un caso de estudio real, era probable que nos encontrarmos con ciertos datos que representaban problemas de calidad de informacion. Los que se identificaron previamente son los siguiente:\n",
    "\n",
    "1. Existe \"Cali, Santa Helena\" y \"Cali, Santa Elena\". *Solucion:* Homologar el nombre a \"Cali, Santa Helena\".\n",
    "2. El codigo del municipio trae una apostrofe \"'\" al principio como mecanismo para convservar el cero del inicio. *Solucion:* Eliminar el apostrofe.\n",
    "3. En algunos casos, el codigo del municipio traia un espacio al inicio. *Solucion:* Quitar ese espacio usando la funcion strip.\n",
    "4. El codigo del departamento trae una apostrofe \"'\" al principio como mecanismo para convservar el cero del inicio. *Solucion:* Eliminar el apostrofe.\n",
    "5. En algunos casos, el codigo del departamento traia un espacio al inicio. *Solucion:* Quitar ese espacio usando la funcion strip.\n",
    "6. La cantidad de kg no estaba en formato de numero.*Solucion:* Cambiar el formato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "df.loc[df[\"Mayorista\"] == \"Cali, Santa Elena\", \"Mayorista\"] = \"Cali, Santa Helena\"\n",
    "\n",
    "#2 \n",
    "df['Codigo_municipio'] = df['Codigo_municipio'].str.replace(\"'\",\"\")\n",
    "\n",
    "#3\n",
    "df['Codigo_municipio'] = df['Codigo_municipio'].str.strip()\n",
    "\n",
    "#4\n",
    "df['Codigo_departamento'] = df['Codigo_departamento'].str.replace(\"'\",\"\")\n",
    "\n",
    "#5\n",
    "df['Codigo_departamento'] = df['Codigo_departamento'].str.strip()\n",
    "\n",
    "#6. error = coerce hace que la funcion no falle cuando se encuentra con valores con nan\n",
    "df['Cant_Kg'] = pd.to_numeric(df['Cant_Kg'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚û°Ô∏è Como el datarame construido esta tan grande, puede que su manipulacion sea tediosa, por lo tanto vamos a construir una bases de datos usando sqllite que nos permita almacenar la informacion.\n",
    "\n",
    "‚ö†Ô∏è **Alertas:** Esta base de datos la vamos a construir fuera de wsl2, ya que este impide la conexion con la base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11764957"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos la ruta en la que vamos a almacenar la informacion.\n",
    "user_path = '/mnt/d/Classes/Data visualization/Mapping data/Database/geo.db'\n",
    "\n",
    "# Creamos un objeto sqlite3 que nos permitira conectarnos con la base de datos\n",
    "conn = sqlite3.connect(user_path)\n",
    "\n",
    "# Enviamos la informacion del dataframe al base de datos\n",
    "df.to_sql('Fact', con=conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Iniciamos con la construccion de los mapas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.1 Establecemos las coordenadas de los centros mayoristas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚û°Ô∏è Como en la base de datos creada no existen las coordenadas de las plazas mayoristas, estas fueron consolidadas de manera manual y estan en la carpeta Data/Static.\n",
    "\n",
    "üóíÔ∏è **Observaciones:** Vamos a enviar toda la informacion que construyamos a la base de datos que justo creamos para aprovechar la facilidad de estructuracion de informacion que ofrece SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos la ruta donde se va a consultar la informacion de las coordenadas de los mayoristas\n",
    "coordenadas_path = \"/home/npalaciosv/Catedra/Geoanalitycs/src/Data/Static/Coordenadas.csv\"\n",
    "\n",
    "# Leemos la informacion dentro de un dataframe.\n",
    "df_coor = pd.read_csv(coordenadas_path,sep=\";\",encoding='utf-8')\n",
    "\n",
    "# Enviamos la informacion a la base de datos creada previamente\n",
    "df_coor.to_sql('Coordenadas_mayoristas', con=conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚û°Ô∏è Vamos a utilizar la libreria Folium para construir los mapas correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciamos un objeto map de folium, centrando la vista inicial del mapa en Bogota\n",
    "map = folium.Map(location=[4.922860659232988, -74.02580517889908], zoom_start=7)\n",
    "\n",
    "# Definimos una ruta en la que va a almacenarce el mapa.\n",
    "maps_path = '/home/npalaciosv/Catedra/Geoanalitycs/src/Graphs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚û°Ô∏è Es hora de crear la primera consulta que nos va a traer la informacion exactamente como la necesitamos para crear el primer layout del mapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos la sentencia sql\n",
    "sentencia_sql = f'''\n",
    "                SELECT \n",
    "                    f.Mayorista,\n",
    "                    cm.Latitud,\n",
    "                    cm.Longitud,\n",
    "                    SUM(f.Cant_Kg) AS Produccion\n",
    "                FROM Fact f\n",
    "                LEFT JOIN Coordenadas_mayoristas cm ON cm.Mayorista = f.Mayorista\n",
    "                WHERE f.Mayorista IS NOT NULL\n",
    "                GROUP BY f.Mayorista,cm.Latitud,cm.Longitud\n",
    "                ORDER BY Produccion DESC\n",
    "                ;\n",
    "                '''\n",
    "\n",
    "# Ejcutamos la sentencia sql y almacenamos el resultado en un dataframe\n",
    "df_may = pd.read_sql_query(sentencia_sql, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos la informacion consultada en listas sobre las que vamos a iterar para agregar elementos al mapa\n",
    "lat = df_may['Latitud'].to_list()\n",
    "lon = df_may['Longitud'].to_list()\n",
    "may = df_may['Mayorista'].to_list()\n",
    "prod = df_may['Produccion'].to_list()\n",
    "\n",
    "# Combinamos las listas con la funcion zip para que podamos iterar con toda la informacion simultaneamente\n",
    "coordenadas = list(zip(lat,lon,may,prod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el icono con el que vamos a representar a los mayoristas dentro del mapa\n",
    "icon_path = '/home/npalaciosv/Catedra/Geoanalitycs/src/Icons/Mayorista.png'\n",
    "\n",
    "# Vamos iterar sobre cada mayorista para localizarlo en el mapa\n",
    "for lat,lon,may,prod in coordenadas:\n",
    "    # Iniciamos el elemento marker desde Folium\n",
    "    folium.Marker(\n",
    "        [lat,lon],\n",
    "        popup=f'{may}',\n",
    "        icon=folium.CustomIcon(icon_image=icon_path,icon_size=(25,25))\n",
    "    ).add_to(map)\n",
    "\n",
    "# Guardamos el mapa generado para efectos de consulta\n",
    "map.save(maps_path+\"mapa.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2 Establecemos las coordenadas de los municipios abastecedores**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚û°Ô∏è Cargamos la informacion que aparece en el archvio Data/Static/Clasificador_geografico.csv. Este archivo contiene las coordenadas de todos los municipios de Colombia.\n",
    "\n",
    "üóíÔ∏è **Observaciones:** Recordemos que la informacion que estamos consultando es oficial, por lo tanto tanto las bases del DANE como el clasificador geografico conservan las mismas llaves üòÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definirmos los tipos de datos espec√≠ficos para las columnas\n",
    "column_types = {'C√≥digo Departamento': str, 'C√≥digo Municipio': str, 'C√≥digo Centro Poblado': str}\n",
    "\n",
    "# Creamos un diccionario que nos permitira homologar los nombres de la base de datos\n",
    "column_rename_map = {\n",
    "    'C√≥digo Departamento': 'Codigo_Departamento',\n",
    "    'C√≥digo Municipio': 'Codigo_Municipio',\n",
    "    'C√≥digo Centro Poblado': 'Codigo_Centro_Poblado'\n",
    "}\n",
    "\n",
    "# Definimos el path de donde se va a traer la informacion del clasificador geografico\n",
    "geopath = '/home/npalaciosv/Catedra/Geoanalitycs/src/Data/Static/Clasificador_geografico.csv'\n",
    "\n",
    "# Creamos un dataframe con la informacion contenida en el clasificador geografico\n",
    "df_geoclas = pd.read_csv(geopath,sep=\";\",encoding='utf-8', dtype=column_types)\n",
    "\n",
    "# Renombramos las columnas correspondientes\n",
    "df_geoclas = df_geoclas.rename(columns=column_rename_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Alertas:** El clasificador geografico trae, para cada municipio, la coordenada de cada uno de sus barrios. Como nosotros solo necesitamos una coordenada, vamos a elegir unicamente la primera que aparezca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1122"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eliminamos las columnas innecesarias\n",
    "df_geoclas = df_geoclas[['Codigo_Departamento','Codigo_Municipio','Longitud','Latitud']]\n",
    "\n",
    "# Dejamos solo una coordenada para cada municipio\n",
    "df_geoclas = df_geoclas.drop_duplicates(subset=['Codigo_Municipio'])\n",
    "\n",
    "# Ajustamos los valors de latitud y longitud para que puedan ser interpretados como numero\n",
    "df_geoclas['Latitud'] = df_geoclas['Latitud'].str.replace(\",\",\".\")\n",
    "df_geoclas['Longitud'] = df_geoclas['Longitud'].str.replace(\",\",\".\")\n",
    "\n",
    "# Enviamos la informacion a una tabla dentro de la base de datos\n",
    "df_geoclas.to_sql('Clasificador_geografico', con=conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Alertas:** Como pudimos ver en la tabla Fact, algunos municipios corresponden a locaciones internacionales, por lo tanto no se se√±ala ninguna coordenada. Manualmente hay creado un archivo con estas coordenadas para poder mapear estos puntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una lista con todos los municipios \"internacionales\"\n",
    "list_coor_int = df_coor_int['Municipio'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üóíÔ∏è **Observaciones:** Para poder llenar las coordenadas vacias dentro de la tabla fact, vamos a cotejarla con las coordendas de los municipios \"internacionales\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traemos la informacion de municipio, longitud y latitud de la tabla Fact\n",
    "sentencia_sql = f'''\n",
    "                SELECT\n",
    "                    DISTINCT f.Codigo_Municipio,\n",
    "                    f.Municipio,\n",
    "                    cg.Latitud,\n",
    "                    cg.Longitud\n",
    "                FROM Fact f\n",
    "                LEFT JOIN Clasificador_geografico cg ON cg.Codigo_Municipio = f.Codigo_Municipio\n",
    "                    ;\n",
    "                '''\n",
    "# Enviamos la informacion a un dataframe\n",
    "df_int = pd.read_sql_query(sentencia_sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üóíÔ∏è **Observaciones:** Leemos el archivo de las coordenadas internacionales para tenerlo como fuente de comparacion y poder rellenar las coordenadas faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el path donde esta el archivo con las coordenadas de los puntos internacionales\n",
    "coordenadas_int_path = \"/home/npalaciosv/Catedra/Geoanalitycs/src/Data/Static/Coordenadas_internacionales.csv\"\n",
    "\n",
    "# Enviamos la informacion a un dataframe\n",
    "df_coor_int = pd.read_csv(coordenadas_int_path,sep=\";\",encoding='utf-8')\n",
    "\n",
    "# Ajustamos los valors de latitud y longitud para que puedan ser interpretados como numero\n",
    "df_coor_int['Latitud'] = df_coor_int['Latitud'].astype(float)\n",
    "df_coor_int['Longitud'] = df_coor_int['Longitud'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1147"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Esta funcion hace que cuando haya un municipio \"internacional\", el dataframe le asigne las coordenadas correspondientes\n",
    "for mun_int in list_coor_int:\n",
    "    df_int.loc[df_int['Municipio']==mun_int,'Latitud'] = df_coor_int.loc[df_coor_int['Municipio']==mun_int,'Latitud'].values\n",
    "    df_int.loc[df_int['Municipio']==mun_int,'Longitud'] = df_coor_int.loc[df_coor_int['Municipio']==mun_int,'Longitud'].values\n",
    "\n",
    "# Deja valores unicos para cada municipio internacional\n",
    "df_int = df_int.dropna()\n",
    "\n",
    "# Enviamos la informacion como una tabla a la base de datos\n",
    "df_int.to_sql('Coordenadas_internacionales', con=conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚û°Ô∏è Ahora, agregamos los elementos correspondientes al mapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el path donde esta el logo que vamos a usar para identificar los municipios productores\n",
    "icon_path = '/home/npalaciosv/Catedra/Geoanalitycs/src/Icons/Productor.png'\n",
    "\n",
    "# Convertimos la informacion consultada en listas sobre las que vamos a iterar para agregar elementos al mapa\n",
    "lat = df_int['Latitud'].to_list()\n",
    "lon = df_int['Longitud'].to_list()\n",
    "municipio = df_int['Municipio'].to_list()\n",
    "\n",
    "# Combinamos las listas con la funcion zip para que podamos iterar con toda la informacion simultaneamente\n",
    "coordenadas = list(zip(lat,lon,municipio))\n",
    "\n",
    "# Vamos iterar sobre cada municipio para localizarlo en el mapa\n",
    "for lat,lon,municipio in coordenadas:\n",
    "    folium.Marker(\n",
    "        [lat,lon],\n",
    "        popup=f'{municipio}',\n",
    "        icon=folium.CustomIcon(icon_image=icon_path,icon_size=(15,15))\n",
    "    ).add_to(map)\n",
    "\n",
    "# Guardamos el mapa generado para efectos de consulta\n",
    "map.save(maps_path+\"mapa.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.3 Coloreamos cada departamento de acuerdo a su capacidad productora**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚û°Ô∏è Para poder tener una delimitacion completa de cada departamento de colombia es necesario consultar la informacion geojson que la representa. En este caso, vamos utilizar esta informacion de referencia üîó https://gist.github.com/john-guerra/43c7656821069d00dcbc.\n",
    "\n",
    "Este archivo esta en Data/Static/colombia.geo.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el path que nos lleva al archivo geojson\n",
    "geopath = '/home/npalaciosv/Catedra/Geoanalitycs/src/Data/Static/colombia.geo.json'\n",
    "\n",
    "# Agregamos el layer con la division departamental al mapa\n",
    "folium.GeoJson(geopath,\n",
    "               style_function= lambda feature : {\n",
    "                        'color': 'black',\n",
    "                        'weight': 2,\n",
    "                        'dashArray': '5, 5',\n",
    "                        'fillOpacity': 0.5,\n",
    "                        }\n",
    "               ).add_to(map)\n",
    "\n",
    "# Guardamos el mapa\n",
    "map.save(maps_path+\"mapa.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una consulta sql que nos va traer la informacion de productividad por departamento\n",
    "sentencia_sql = f'''\n",
    "                SELECT\n",
    "                    f.Codigo_departamento AS DPTO,\n",
    "                    SUM(f.Cant_Kg) AS Produccion\n",
    "                FROM Fact f\n",
    "                WHERE f.Codigo_departamento != 'n.a.'\n",
    "                GROUP BY f.Codigo_departamento\n",
    "                ORDER BY Produccion DESC\n",
    "                ;\n",
    "                '''\n",
    "\n",
    "# Guardamos la informacion en un dataframe\n",
    "df_temp = pd.read_sql_query(sentencia_sql, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'DPTO': '25', 'Produccion': 6786107919.296},\n",
       " {'DPTO': '15', 'Produccion': 4400427211.527},\n",
       " {'DPTO': '05', 'Produccion': 3804643763.755},\n",
       " {'DPTO': '68', 'Produccion': 2730609255.394},\n",
       " {'DPTO': '76', 'Produccion': 2481002743.13},\n",
       " {'DPTO': '52', 'Produccion': 2423965237.539},\n",
       " {'DPTO': '50', 'Produccion': 2326006609.249},\n",
       " {'DPTO': '54', 'Produccion': 1492876634.912},\n",
       " {'DPTO': '11', 'Produccion': 1481709028.603},\n",
       " {'DPTO': '73', 'Produccion': 1290401140.915},\n",
       " {'DPTO': '63', 'Produccion': 872649277.478},\n",
       " {'DPTO': '23', 'Produccion': 820047330.802},\n",
       " {'DPTO': '41', 'Produccion': 774962367.911},\n",
       " {'DPTO': '17', 'Produccion': 708300483.243},\n",
       " {'DPTO': '81', 'Produccion': 526311244.624},\n",
       " {'DPTO': '08', 'Produccion': 510248712.027},\n",
       " {'DPTO': '47', 'Produccion': 452783227.959},\n",
       " {'DPTO': '19', 'Produccion': 450456382.505},\n",
       " {'DPTO': '66', 'Produccion': 412911803.823},\n",
       " {'DPTO': '13', 'Produccion': 293710118.93},\n",
       " {'DPTO': '85', 'Produccion': 261060042.626},\n",
       " {'DPTO': '20', 'Produccion': 191444027.288},\n",
       " {'DPTO': '00', 'Produccion': 178878142.966},\n",
       " {'DPTO': '70', 'Produccion': 90142760.349},\n",
       " {'DPTO': '18', 'Produccion': 85483678.836},\n",
       " {'DPTO': '44', 'Produccion': 55930437.602},\n",
       " {'DPTO': '86', 'Produccion': 30582653.549},\n",
       " {'DPTO': '27', 'Produccion': 30024953.03},\n",
       " {'DPTO': '95', 'Produccion': 18966891.128},\n",
       " {'DPTO': '91', 'Produccion': 8058411.7},\n",
       " {'DPTO': '99', 'Produccion': 1644849.3},\n",
       " {'DPTO': '94', 'Produccion': 225692.0},\n",
       " {'DPTO': '97', 'Produccion': 17065.0},\n",
       " {'DPTO': '88', 'Produccion': 7500.0}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convertimos el DataFrame a un diccionario\n",
    "temp_dict = df_temp.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para asignar color basado en la producci√≥n\n",
    "def asignar_color(feature):\n",
    "\n",
    "    # Calcular el m√≠nimo y m√°ximo de las producciones\n",
    "    producciones = [item['Produccion'] for item in temp_dict]\n",
    "    produccion_minima = min(producciones)\n",
    "    produccion_maxima = max(producciones)*0.4 # Este valor esta castigado para mejorar un poco la colorimetria del mapa\n",
    "\n",
    "    # Definir colores en la escala\n",
    "    color_minimo = 'red'\n",
    "    color_medio = 'orange'\n",
    "    color_maximo = 'green'\n",
    "\n",
    "    # Extramos el codigo del departamento que esta contenido en el archivo geojson\n",
    "    codigo_departamento = feature['properties'].get('DPTO')\n",
    "\n",
    "    # Obtenemos la producci√≥n de la consulta que guardamos en df_temp o 0 si no est√° definido\n",
    "    produccion = next((item['Produccion'] for item in temp_dict if item['DPTO'] == codigo_departamento), 0)\n",
    "\n",
    "    # Normalizamos la producci√≥n entre 0 y 1\n",
    "    norm = mcolors.Normalize(vmin=produccion_minima, vmax=produccion_maxima)\n",
    "\n",
    "    # Crear una interpolaci√≥n lineal de colores\n",
    "    color_interp = mcolors.LinearSegmentedColormap.from_list('custom_map', [color_minimo, color_medio, color_maximo])\n",
    "\n",
    "    # Obtener el color seg√∫n la producci√≥n normalizada\n",
    "    color = mcolors.rgb2hex(color_interp(norm(produccion)))\n",
    "\n",
    "    return color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregamos la division de los mapas coloreados segun su productividad\n",
    "folium.GeoJson(geopath,\n",
    "               style_function= lambda feature : {\n",
    "                        'fillColor':asignar_color(feature),\n",
    "                        'color': 'black',\n",
    "                        'weight': 2,\n",
    "                        'dashArray': '5, 5',\n",
    "                        'fillOpacity': 0.5,\n",
    "                        }\n",
    "               ).add_to(map)\n",
    "\n",
    "# Guardamos el mapa\n",
    "map.save(maps_path+\"mapa.html\")\n",
    "\n",
    "# Cerramos la conexion\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoanalitics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
